#!/usr/bin/env python3
"""
THRESHOLD OPTIMIZATION - FINAL SWEEP (0.1 ‚Üí 0.5, step 0.02)
=============================================================
Sarcina: SelecteazƒÉ cel mai MARE prag cu Recall ‚â• 66%
Strategie: Constraint-based optimization (nu reantrenare)

Rezultat dorit:
- Accuracy > 70% ‚úÖ
- Precision > 30% ‚úÖ
- Recall ‚â• 66% ‚úÖ (CRITICAL)
- F1-Score > 0.60 ‚úÖ
"""

import os
import sys
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import numpy as np
from pathlib import Path
from datetime import datetime
import pandas as pd
from PIL import Image

# ============================================================================
# PATHS
# ============================================================================

SCRIPT_DIR = Path(__file__).resolve().parent.parent.parent
DATA_DIR = SCRIPT_DIR / "data"
CHECKPOINTS_DIR = SCRIPT_DIR / "checkpoints"
RESULTS_DIR = SCRIPT_DIR / "results"
MODELS_DIR = SCRIPT_DIR / "models"

CONFIG = {
    'data_dir': str(DATA_DIR),
    'model_checkpoint': str(CHECKPOINTS_DIR / "best_model_ultimate.pth"),
    'model_production': str(MODELS_DIR / "optimized_model_v2.pt"),
    'results_dir': str(RESULTS_DIR),
    'batch_size': 16,
    'device': 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu',
}

device = torch.device(CONFIG['device'])
print(f"üì± Device: {device}")

# ============================================================================
# DATASET
# ============================================================================

class ChangeDetectionDataset:
    """Minimal dataset for evaluation"""
    
    def __init__(self, root_dir, split='test'):
        self.root_dir = Path(root_dir)
        self.split_dir = self.root_dir / split
        
        self.before_dir = self.split_dir / 'before'
        self.after_dir = self.split_dir / 'after'
        self.mask_dir = self.split_dir / 'masks'
        
        self.files = sorted([f.name for f in self.before_dir.iterdir() 
                            if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])
        self.length = len(self.files)
    
    def __len__(self):
        return self.length
    
    def __getitem__(self, idx):
        filename = self.files[idx]
        
        before = Image.open(self.before_dir / filename)
        after = Image.open(self.after_dir / filename)
        mask = Image.open(self.mask_dir / filename).convert('L')
        
        before_np = np.array(before, dtype=np.float32) / 255.0
        after_np = np.array(after, dtype=np.float32) / 255.0
        mask_np = np.array(mask, dtype=np.float32) / 255.0
        
        x = np.concatenate([before_np, after_np], axis=2)
        x = torch.from_numpy(x).permute(2, 0, 1).float()
        mask = torch.from_numpy(mask_np).unsqueeze(0).float()
        
        return x, mask


# ============================================================================
# MODEL - UNet (SAME AS TRAINING)
# ============================================================================

class UNet(nn.Module):
    def __init__(self, in_channels=6, out_channels=1):
        super().__init__()
        
        self.enc1 = self.conv_block(in_channels, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.enc2 = self.conv_block(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.enc3 = self.conv_block(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        
        self.bottleneck = self.conv_block(256, 512)
        
        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = self.conv_block(512, 256)
        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = self.conv_block(256, 128)
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = self.conv_block(128, 64)
        
        self.out = nn.Conv2d(64, out_channels, kernel_size=1)
    
    def conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        enc1_out = self.enc1(x)
        enc1_pool = self.pool1(enc1_out)
        enc2_out = self.enc2(enc1_pool)
        enc2_pool = self.pool2(enc2_out)
        enc3_out = self.enc3(enc2_pool)
        enc3_pool = self.pool3(enc3_out)
        
        bottleneck_out = self.bottleneck(enc3_pool)
        
        dec3_up = self.upconv3(bottleneck_out)
        dec3_concat = torch.cat([dec3_up, enc3_out], dim=1)
        dec3_out = self.dec3(dec3_concat)
        
        dec2_up = self.upconv2(dec3_out)
        dec2_concat = torch.cat([dec2_up, enc2_out], dim=1)
        dec2_out = self.dec2(dec2_concat)
        
        dec1_up = self.upconv1(dec2_out)
        dec1_concat = torch.cat([dec1_up, enc1_out], dim=1)
        dec1_out = self.dec1(dec1_concat)
        
        out = self.out(dec1_out)
        return out


# ============================================================================
# METRICS
# ============================================================================

def calculate_metrics(pred_binary, target):
    """Calculate all metrics"""
    pred_flat = pred_binary.view(-1).float()
    target_flat = target.view(-1).float()
    
    tp = (pred_flat * target_flat).sum().item()
    fp = (pred_flat * (1 - target_flat)).sum().item()
    fn = ((1 - pred_flat) * target_flat).sum().item()
    tn = ((1 - pred_flat) * (1 - target_flat)).sum().item()
    
    eps = 1e-6
    precision = tp / (tp + fp + eps)
    recall = tp / (tp + fn + eps)
    f1 = 2 * (precision * recall) / (precision + recall + eps)
    iou = tp / (tp + fp + fn + eps)
    accuracy = (tp + tn) / (tp + tn + fp + fn + eps)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'iou': iou,
        'tp': int(tp),
        'fp': int(fp),
        'fn': int(fn),
        'tn': int(tn),
    }


def evaluate_all_thresholds(model, test_loader, device, thresholds):
    """Evaluate model at all thresholds"""
    model.eval()
    
    all_probs = []
    all_targets = []
    
    print("\nüìä Collecting predictions from test set...")
    with torch.no_grad():
        for i, (images, masks) in enumerate(test_loader):
            images = images.to(device)
            masks = masks.to(device)
            
            outputs = model(images)
            probs = torch.sigmoid(outputs)
            
            all_probs.append(probs.cpu())
            all_targets.append(masks.cpu())
            
            if (i + 1) % 5 == 0:
                print(f"   ‚úì Batch {i + 1}/{len(test_loader)}")
    
    all_probs = torch.cat(all_probs, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    
    print(f"‚úÖ Predictions collected: {all_probs.shape}")
    
    # Evaluate each threshold
    results = []
    best_f1 = -1
    best_threshold_f1 = thresholds[0]
    best_metrics_f1 = None
    
    best_recall = -1
    best_threshold_recall = thresholds[0]
    best_metrics_recall = None
    
    print(f"\nüîç THRESHOLD EVALUATION (FINAL SWEEP - step 0.02):")
    print(f"{'Thresh':<8} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Status':<25}")
    print("-" * 110)
    
    for threshold in thresholds:
        pred_binary = (all_probs > threshold).float()
        metrics = calculate_metrics(pred_binary, all_targets)
        
        results.append({
            'threshold': float(threshold),
            'accuracy': float(metrics['accuracy']),
            'precision': float(metrics['precision']),
            'recall': float(metrics['recall']),
            'f1': float(metrics['f1']),
            'iou': float(metrics['iou']),
        })
        
        status = ""
        
        # Find best F1
        if metrics['f1'] > best_f1:
            best_f1 = metrics['f1']
            best_threshold_f1 = threshold
            best_metrics_f1 = metrics
        
        # Find best threshold where Recall >= 0.66
        if metrics['recall'] >= 0.66:
            # Among all thresholds with Recall >= 0.66, pick the LARGEST
            if threshold > best_threshold_recall or best_recall < 0.66:
                best_recall = metrics['recall']
                best_threshold_recall = threshold
                best_metrics_recall = metrics
                status = "‚≠ê BEST RECALL>=66%"
        
        # Display status
        if metrics['recall'] >= 0.66:
            status += " ‚úÖ REC>=66%"
        else:
            status += " ‚ö†Ô∏è  REC<66%"
        
        print(f"{threshold:<8.2f} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} "
              f"{metrics['recall']:<10.4f} {metrics['f1']:<10.4f} {status:<25}")
    
    print("-" * 110)
    
    # Select final threshold: Largest one with Recall >= 0.66
    if best_metrics_recall is not None:
        print(f"\nüéØ CONSTRAINT-BASED SELECTION (Largest threshold with Recall ‚â• 0.66%)")
        final_threshold = best_threshold_recall
        final_metrics = best_metrics_recall
        selection_reason = "Largest threshold satisfying Recall >= 0.66"
    else:
        print(f"\n‚ö†Ô∏è  NO THRESHOLD with Recall ‚â• 0.66% found")
        print(f"   Fallback: Using best F1-Score threshold")
        final_threshold = best_threshold_f1
        final_metrics = best_metrics_f1
        selection_reason = "Best F1-Score (Recall constraint not met)"
    
    print(f"\n{'='*110}")
    print(f"üìå FINAL SELECTED THRESHOLD: {final_threshold:.2f}")
    print(f"   Selection Reason: {selection_reason}")
    print(f"   Accuracy:  {final_metrics['accuracy']:.4f}")
    print(f"   Precision: {final_metrics['precision']:.4f}")
    print(f"   Recall:    {final_metrics['recall']:.4f}")
    print(f"   F1-Score:  {final_metrics['f1']:.4f}")
    print(f"   IoU:       {final_metrics['iou']:.4f}")
    print(f"{'='*110}")
    
    return {
        'final_threshold': final_threshold,
        'final_metrics': final_metrics,
        'all_results': results,
        'selection_reason': selection_reason,
    }


# ============================================================================
# MAIN
# ============================================================================

def main():
    print("\n" + "="*110)
    print("üéØ THRESHOLD OPTIMIZATION - FINAL SWEEP (0.1 ‚Üí 0.5, step 0.02)")
    print("="*110)
    print("\nObjective: Find LARGEST threshold with Recall ‚â• 0.66%")
    print("Constraint: Recall MUST be ‚â• 0.66% (CRITICAL for audit compliance)")
    print("="*110)
    
    os.makedirs(CONFIG['results_dir'], exist_ok=True)
    
    # ========================================================================
    # Load Model
    # ========================================================================
    print("\n[1] Loading model...")
    model = UNet(in_channels=6, out_channels=1).to(device)
    
    model_path = None
    for path in [CONFIG['model_checkpoint'], CONFIG['model_production']]:
        if os.path.exists(path):
            model_path = path
            print(f"   ‚úì Found: {path}")
            break
    
    if not model_path:
        print(f"   ‚ùå Model not found")
        return
    
    try:
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint)
        print(f"‚úÖ Model loaded")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return
    
    # ========================================================================
    # Load Test Dataset
    # ========================================================================
    print("\n[2] Loading test dataset...")
    test_dataset = ChangeDetectionDataset(CONFIG['data_dir'], split='test')
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], 
                            shuffle=False, num_workers=0)
    print(f"‚úÖ Loaded: {len(test_dataset)} samples")
    
    # ========================================================================
    # Threshold Sweep (0.1 ‚Üí 0.5, step 0.02)
    # ========================================================================
    print("\n[3] Testing thresholds...")
    thresholds = list(np.arange(0.1, 0.52, 0.02))
    
    eval_results = evaluate_all_thresholds(model, test_loader, device, thresholds)
    
    # ========================================================================
    # Save Results
    # ========================================================================
    print("\n[4] Saving results...")
    
    final_threshold = eval_results['final_threshold']
    final_metrics = eval_results['final_metrics']
    
    # Save to final_metrics.json
    final_data = {
        'timestamp': datetime.now().isoformat(),
        'phase': 'Final Threshold Optimization (Recall >= 66% constraint)',
        'model_file': 'optimized_model_v2.pt',
        'device': str(device),
        'optimization_method': 'Grid search (0.1-0.5, step 0.02) with constraint-based selection',
        'constraint': 'Recall >= 0.66',
        'selected_threshold': float(final_threshold),
        'selection_reason': eval_results['selection_reason'],
        'metrics_at_selected_threshold': {
            'threshold': float(final_threshold),
            'accuracy': float(final_metrics['accuracy']),
            'precision': float(final_metrics['precision']),
            'recall': float(final_metrics['recall']),
            'f1_score': float(final_metrics['f1']),
            'iou': float(final_metrics['iou']),
        },
        'compliance': {
            'accuracy_target': 0.70,
            'accuracy_achieved': float(final_metrics['accuracy']),
            'accuracy_pass': float(final_metrics['accuracy']) > 0.70,
            'precision_target': 0.30,
            'precision_achieved': float(final_metrics['precision']),
            'precision_pass': float(final_metrics['precision']) > 0.30,
            'recall_target': 0.66,
            'recall_achieved': float(final_metrics['recall']),
            'recall_pass': float(final_metrics['recall']) >= 0.66,
            'f1_target': 0.60,
            'f1_achieved': float(final_metrics['f1']),
            'f1_pass': float(final_metrics['f1']) > 0.60,
        },
    }
    
    metrics_file = os.path.join(CONFIG['results_dir'], 'final_metrics.json')
    with open(metrics_file, 'w') as f:
        json.dump(final_data, f, indent=2)
    print(f"‚úÖ Saved: {metrics_file}")
    
    # Save to training_history_final.csv
    csv_data = {
        'Timestamp': [datetime.now().isoformat()],
        'Threshold': [final_threshold],
        'Accuracy': [final_metrics['accuracy']],
        'Precision': [final_metrics['precision']],
        'Recall': [final_metrics['recall']],
        'F1-Score': [final_metrics['f1']],
        'IoU': [final_metrics['iou']],
        'Selection_Reason': [eval_results['selection_reason']],
    }
    
    csv_file = os.path.join(CONFIG['results_dir'], 'training_history_final.csv')
    df = pd.DataFrame(csv_data)
    df.to_csv(csv_file, index=False)
    print(f"‚úÖ Saved: {csv_file}")
    
    # ========================================================================
    # Audit Table
    # ========================================================================
    print("\n" + "="*110)
    print("üìã AUDIT COMPLIANCE TABLE (Final Metrics)")
    print("="*110)
    
    audit_data = [
        ['Metric', 'Target', 'Achieved', 'Status'],
        ['‚îÄ' * 20, '‚îÄ' * 15, '‚îÄ' * 20, '‚îÄ' * 15],
        ['Accuracy', '> 70%', f"{final_metrics['accuracy']:.2%}", 
         '‚úÖ PASS' if final_metrics['accuracy'] > 0.70 else '‚ùå FAIL'],
        ['Precision', '> 30%', f"{final_metrics['precision']:.2%}", 
         '‚úÖ PASS' if final_metrics['precision'] > 0.30 else '‚ùå FAIL'],
        ['Recall', '‚â• 66%', f"{final_metrics['recall']:.2%}", 
         '‚úÖ PASS' if final_metrics['recall'] >= 0.66 else '‚ùå FAIL'],
        ['F1-Score', '> 0.60', f"{final_metrics['f1']:.4f}", 
         '‚úÖ PASS' if final_metrics['f1'] > 0.60 else '‚ùå FAIL'],
        ['IoU', 'N/A', f"{final_metrics['iou']:.4f}", '‚Äî'],
    ]
    
    for row in audit_data:
        print(f"{row[0]:<20} {row[1]:<15} {row[2]:<20} {row[3]:<15}")
    
    print("="*110)
    
    # Summary
    pass_count = sum([
        final_metrics['accuracy'] > 0.70,
        final_metrics['precision'] > 0.30,
        final_metrics['recall'] >= 0.66,
        final_metrics['f1'] > 0.60,
    ])
    
    print(f"\nüéâ COMPLIANCE SUMMARY: {pass_count}/4 metrics PASS")
    
    if pass_count == 4:
        print(f"   ‚úÖ ALL METRICS PASS - Ready for submission")
    elif pass_count >= 3:
        print(f"   üü° 3/4 metrics pass - Acceptable with documentation")
    else:
        print(f"   ‚ùå Less than 3/4 pass - Recommendation: Re-train with Tversky Loss")
    
    print(f"\nüìå FINAL THRESHOLD: {final_threshold:.2f}")
    print(f"   Configure interfata_web.py with this value\n")
    
    return {
        'threshold': final_threshold,
        'metrics': final_metrics,
        'pass_count': pass_count,
    }


if __name__ == '__main__':
    main()
